{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheat Sheet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0. Introduction:**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2020: 40 zettabtyes of data ($40 * 10^9$ terabytes)\n",
    "- DS: \"the journey of extracting knowledge from data\", \"From collecting data to doing something useful with it\".\n",
    "- Data Mining = Data science + machine learning + visualization\n",
    "- “Machine learning is the science of making computers learn and act like humans by feeding data and information without explicitly being programmed”\n",
    "- Triangle of Data and Knowledge\n",
    "  - Data = simple, isolated facts (e.g. values in a database table)\n",
    "  - Information = data in context\n",
    "  - Knowledge = interpreted information\n",
    "  - Intelligence = use of knowledge to choose between alternatives\n",
    "  - Wisdom = intelligence + experience (guided by values and commitment)\n",
    "- Dataset size: Columns (features) x Rows (instances/observations)\n",
    "  -Data gathering\n",
    "  - Scraping\n",
    "  - APIs\n",
    "    - JSON (JavaScript Object Notation)\n",
    "    - IMDB\n",
    "    - Kaggle\n",
    "  - Tools for data collection\n",
    "    - beautifulsoup (HTML)\n",
    "- **GDPR:**\n",
    "  - All data thats collected within the EU is protected by the GDPR\n",
    "  - Personal data = any information relating to an identified or identifiable natural person (data subject)\n",
    "    - Name, ID number, religion, political views, location data, online identifier, etc.\n",
    "    - Data controllers must notify the authorities of any data breach within 72 hours\n",
    "    - Consent must be: clear, freely given, informed, and specific\n",
    "  - Data rights:\n",
    "    - Right to be informed\n",
    "    - Right to be forgotten\n",
    "    - Right to rectification (=)\n",
    "    - Right to restrict processing\n",
    "    - Data portability (Individuals can request their personal information. Companies need to reply in 30 days )\n",
    "    - Right to object or appeal to automated decision making\n",
    "- **Data types:**\n",
    "  - Structured data:\n",
    "    - Categorical\n",
    "      - Nominal (not orderable: gender, ID, zip code,)\n",
    "      - Ordinal (orderable: e.g. grades, low-med-high)\n",
    "    - Numerical\n",
    "      - Continuous (distance, time, temperature, weight, height)\n",
    "      - Discrete (countable: number of children, number of cars)\n",
    "  - Unstructured or semistructued:\n",
    "    - Text\n",
    "    - Multimedia (image, audio, video)\n",
    "    - XML/JSON\n",
    "- **Metrics for data readiness:**\n",
    "  - reliability, accuracy, accessibility, security, privacy, consistency validity, timeliness, granularity\n",
    "- Converting Continuous data to Discrete variables:\n",
    "  - Convert incomes to income groups\n",
    "- Typical numerical transformations:\n",
    "  - Logarithmic transformation $log(x)$\n",
    "  - 0-1 Normalization $x_{new} = (x_i - min(x)/max(x)-min(x))$\n",
    "  - Z Normalization $z= (x_i - mean(x)/std(x))$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Regression:**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is a statistical method that helps you understand the relationship between dependent and independent variables. It's mainly used for prediction and forecasting.\n",
    "\n",
    "- Predicting numerical values (age, price, temperature, etc.)\n",
    "- Supervised learning\n",
    "- Residual Sum of Squares = $\\sum(y-y')^2$, where y=actual_value, y'=predicted value\n",
    "- How to find the best fitted line = _Gradient Descent_\n",
    "  - Important to scale features! Might not work well when not scaled. Squished circle vs Normal circle.\n",
    "  - Iterative approach that tries to reduce the RSS\n",
    "- **Goodness of fit**\n",
    "  - RSS = Residual Sum of Squares -> $\\sum(y-y')^2$, where y=actual_value, y'=predicted value\n",
    "  - TSS = Total Sum of Squares -> $\\sum(y-\\bar{y})^2$ where y=actual_value, $\\bar{y}$=mean\n",
    "  - $R^2$ : 1 = perfect fit, 0 = same as avg.\n",
    "  - $R^2 = 1 - RSS/TSS$\n",
    "- **MAE & MSE:**\n",
    "\n",
    "  - are metrics to evaluate the regression model. MAE is the average of the absolute differences between the predicted and actual values. MSE is the average of the squared differences. MSE penalizes larger errors more than MAE. Eg if MAE = 25’000 it means that on average we are off by 25k CHF in our predictions.\n",
    "\n",
    "  - **MAE (Mean Absolute Error):**\n",
    "    - $1/n * Σ|yi - y'|$\n",
    "  - **MSE (Mean Squared Error):**\n",
    "    - $1/n * Σ(yi - y')^2$\n",
    "  - $Model: y = β0 + β1*x + β2*x^2 + ... + βd*x^d + ε$\n",
    "  -\n",
    "\n",
    "- **Multivariate linear regression:**\n",
    "\n",
    "  - Here, you try to predict a dependent variable based on two or more independent variables. For example, predicting a house's price based on the number of bedrooms and the size in square feet. The formula is y = β0 + β1*x1 + β2*x2 + ... + βn\\*xn + ε, where y is the dependent variable, x1, x2, ..., xn are the independent variables, β0, β1, ..., βn are the coefficients, and ε is the error term.\n",
    "  - $Model: y = β0 + β1*x1 + β2*x2 + ... + βn*xn + ε$\n",
    "\n",
    "- **Polynomial regression:**\n",
    "\n",
    "  - This is a form of regression where the relationship between the independent and the dependent variables is modelled as an nth degree polynomial. It is useful when the relationship is not linear but rather curvilinear. For example, predicting crop yield based on temperature, where the yield might increase with temperature to a point, but then decrease with further temperature increase.\n",
    "\n",
    "- **Regularization (Lasso, Ridge):**\n",
    "\n",
    "  - Regularization helps to prevent overfitting by adding a penalty term to the loss function. Ridge regression (L2 regularization) minimizes the sum of the squares of the coefficients, while Lasso (L1 regularization) minimizes the sum of the absolute values of the coefficients. This penalty term encourages smaller coefficients, which leads to simpler models.\n",
    "  - Lasso (L1 regularization): Sum of absolute values of coefficients\n",
    "  - Ridge (L2 regularization): Sum of squares of coefficients\n",
    "\n",
    "- **Cross Validation:**\n",
    "\n",
    "  - Using K-fold (5 or 10 usually) to get a more accurate picture of the error.\n",
    "\n",
    "- **Regression with categorical values:**\n",
    "  - Binary data (gender) = convert to [0,1]\n",
    "  - 1-hot encoding = > 2 values (convert each value to a column). ROT: 10-15\n",
    "  - Label-encoding = > 2 values (give each value a label and create a register) ROT: > 15\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Classification:**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised learning\n",
    "- Classification involves predicting a categorical (something discrete) label for an input. Useful to detect language, decide whether to give a loan or not, classify sentiment.\n",
    "- Problems that can be solved by a linear classifier are called _linearly separable_.\n",
    "- Default rate:\n",
    "\n",
    "  - size (most_common_class) / size(dataset)\n",
    "  - For binary classes: 1/N, where N is number of classes.\n",
    "\n",
    "- **Logistic Regression:**\n",
    "- $Logistic(x)=1/(1+e^{-x})$\n",
    "- This is a binary classification method that models the probability that an instance belongs to the default class. For example, it can be used to predict whether an email is spam (1) or not spam (0). Goes from +inf to -inf.\n",
    "- $log(p/1-p) = β0 + β1*x1 + β2*x2 + ... + βn*xn$\n",
    "- **Confusion Matrix:**\n",
    "\n",
    "|               | Predicted Positive | Predicted Negative |\n",
    "| ------------- | ------------------ | ------------------ |\n",
    "| True Positive | TP                 | FN                 |\n",
    "| True Negative | FP                 | TN                 |\n",
    "\n",
    "Where:\n",
    "\n",
    "- TP = True Positives: The cases in which the model predicted Positive, and the truth is also Positive.\n",
    "- FP = False Positives (Type I error): The cases in which the model predicted Positive, but the truth is Negative.\n",
    "- FN = False Negatives (Type II error): The cases in which the model predicted Negative, but the truth is Positive.\n",
    "- TN = True Negatives: The cases in which the model predicted Negative, and the truth is also Negative.\n",
    "\n",
    "  - This is a table that describes the performance of a classification model. It includes True Positives (actual positive and predicted positive), True Negatives (actual negative and predicted negative), False Positives (actual negative but predicted positive), and False Negatives (actual positive but predicted negative).\n",
    "  - **Accuracy:**\n",
    "    - Accuracy is the ratio of correctly predicted instances to the total instances. Overall, how often is the model correct?\n",
    "    - $(TP + TN) / (TP + TN + FP + FN)$\n",
    "  - **Precision:**\n",
    "    - Precision is the ratio of correctly predicted positive instances to the total predicted positives. When the model predicts Positive, how often is it correct?\n",
    "    - $TP / (TP + FP)$\n",
    "  - **Recall:**\n",
    "    - Recall (or Sensitivity) is the ratio of correctly predicted positive instances to all actual positives. When the truth is Positive, how often does the model predict it?\n",
    "    - $TP / (TP + FN)$\n",
    "  - **Misclassification Rate (Error Rate):**\n",
    "    - Overall, how often is the model wrong?\n",
    "    - $Error Rate = (FP+FN) / (TP+FP+FN+TN)$\n",
    "  - **F1 Score:**\n",
    "    - F1 Score is the harmonic mean of Precision and Recall It's useful when the data has imbalanced classes.\n",
    "    - $2*(Precision*Recall)/(Precision + Recall)$\n",
    "\n",
    "**Lets use the following matrix to do calculations:**\n",
    "\n",
    "|              | Predicted Class 1 | Predicted Class 2 | Predicted Class 3 |\n",
    "| ------------ | ----------------- | ----------------- | ----------------- |\n",
    "| True Class 1 | 50                | 10                | 5                 |\n",
    "| True Class 2 | 20                | 60                | 5                 |\n",
    "| True Class 3 | 10                | 10                | 70                |\n",
    "\n",
    "1. For class 1:\n",
    "\n",
    "   - Precision for Class 1 = TP / (TP + FP) = 50 / (50 + 30) = 0.625\n",
    "   - Recall for Class 1 = TP / (TP + FN) = 50 / (50 + 15) = 0.77\n",
    "   - F1 Score for Class 1 = 2 _ (Precision _ Recall) / (Precision + Recall) = 2 _ (0.625 _ 0.77) / (0.625 + 0.77) = 0.69\n",
    "   - Accuracy for Class 1 = (TP + TN) / (TP + TN + FP + FN) = (50 + 160) / (50 + 160 + 30 + 15) = 0.84\n",
    "   - Error rate for Class 1 = 1 - Accuracy = 1 - 0.84 = 0.16\n",
    "\n",
    "2. For the entire confusion matrix:\n",
    "\n",
    "   First, let's calculate overall TP, FP, FN, and TN:\n",
    "\n",
    "   - Total TP = Sum of diagonal elements = 50 + 60 + 70 = 180\n",
    "   - Total FP = Sum of all non-diagonal elements in predicted columns = 30 (from Class 1 column) + 20 (from Class 2 column) + 10 (from Class 3 column) = 60\n",
    "   - Total FN = Sum of all non-diagonal elements in true class rows = 15 (from Class 1 row) + 25 (from Class 2 row) + 20 (from Class 3 row) = 60\n",
    "   - Total TN = Sum of all elements - TP - FP - FN = 300 - 180 - 60 - 60 = 0 (this makes sense, because in multi-class problems, an instance can either be a TP, FP, or FN for a class)\n",
    "   - Accuracy for the matrix = (Total TP + Total TN) / (Total TP + Total TN + Total FP + Total FN)\n",
    "\n",
    "**ROC Curve:**\n",
    "\n",
    "- ROC curve is a plot of True Positive Rate (Recall) vs False Positive Rate (1 - Specificity) for different classification thresholds. It shows the tradeoff between sensitivity and specificity. (TP: Y-axis. FP: X-axis). FYI: random model: ROC @ 0.5. Diagonal line from 0,0 to 1,1.\n",
    "- ROC curve is useful when the classes are imbalanced.\n",
    "- ROC curves answers the question: what will happen if we move the prediction line up and down?\n",
    "- Advantages of ROC-curve: (1) Visualize TPR and TNR. (2) Compare different models.\n",
    "\n",
    "**kNN (k-Nearest Neighbors) Classification:**\n",
    "\n",
    "- Odd numbers in KNN: avoid ties.\n",
    "- Distance types:\n",
    "  - (1) Euclidean (L2): captures the distance between two points in a plane (Pythagorean theorem) $d(p,q) = \\sqrt{(q_1-p_1)^2+(q_2-p_2)^2}$.\n",
    "  - (2) Manhattan (L1): captures the distance between two points in a plane (sum of absolute differences) $d(x,y) = \\sum_{i=1}^{n}|x_i-y_i|$.\n",
    "- Euclidean distance is more sensitive to outliers than Manhattan distance. Often used when the data is dense or continuous. Manhattan distance is often used when the data is sparse or discrete.\n",
    "- We pick the right K by using cross-validation. Using ONLY the training data.\n",
    "\n",
    "**Decision Trees:**\n",
    "\n",
    "- Decision trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "- Decision trees are easy to interpret and visualize.\n",
    "- Use cases: decide prescriptions, decide whether to approve a loan, decide whether to hire a candidate, etc.\n",
    "- How to grow a Decision Tree:\n",
    "  - (1) Start at the root node with all observations\n",
    "  - (2) Pick attributes and value/threshold to get new nodes.\n",
    "    - Split should produce pure and equally balanced nodes\n",
    "    - Select the feature that leads to lowest classification error.\n",
    "  - (3) When to stop? Purity, less than x points...\n",
    "- How to find the optimal tree depth: cross validation\n",
    "- Entropy: measure of impurity in a bunch of examples. $H(X) = -\\sum_{i=1}^{n}p(x_i)log_2p(x_i)$. The information gain is then defined as $IG(T,a) = H(T) - H(T|a)$.\n",
    "  - Calculation steps:\n",
    "    - (1) Calculate entropy of parent node -> $H(T)$.\n",
    "    - (2) Calculate entropy of each individual node -> $H(T|a)$.\n",
    "    - (3) Calculate information gain -> $IG(T,a) = H(T) - H(T|a)$.\n",
    "    - (4) Select the feature that leads to highest information gain.\n",
    "\n",
    "<image  src='./illustrations/entropy_inf_gain.png' height='200'/>\n",
    "\n",
    "**Random Forests:**\n",
    "\n",
    "- Random forests are an ensemble (classifier) learning method for classification and regression. It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "- Random forests are a bagging (bootstrap aggregating) method.\n",
    "- Steps to build a random forest:\n",
    "  - (1) Randomly select k features from total m features where k << m\n",
    "  - (2) Among the k features, calculate the node d using the best split point\n",
    "  - (3) Split the node into daughter nodes using the best split\n",
    "  - (4) Repeat steps 1 to 3 until leaf nodes are finalized\n",
    "  - (5) Build forest by repeating steps 1 to 4 for n times to create n number of trees\n",
    "  - (6) Select the feature that leads to lowest classification error.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Clustering:**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups.\n",
    "- No labels, discover structure by grouping\n",
    "- Unsupervised learning\n",
    "- **K-Means:**\n",
    "  - Runtime complexity: $O(ktn)$ where k is the number of clusters, t is the number of iterations, and n is the number of data points.\n",
    "  - This algorithm divides a set of samples into disjoint clusters. Each cluster is described by the mean of the samples in the cluster. For example, you might cluster customers into k groups based on their shopping behavior.\n",
    "  - Initialize k centroids randomly\n",
    "  - Assign each point to the nearest centroid\n",
    "  - Recalculate centroids\n",
    "  - Repeat until convergence\n",
    "  - How to find the right amount of clusters: elbow method\n",
    "- **Hierarchical clustering:**\n",
    "  - Runtime complexity: At least $O(n^2)$ where n is the number of data points (single-linkage)\n",
    "  - This algorithm builds a hierarchy of clusters by creating a cluster tree or dendrogram. It can either start with all individual instances and merge them into clusters (_agglomerative_ : bottom-up), or start with a single cluster and divide it up (_divisive_ : top-down).\n",
    "  - Start with each point as a separate cluster\n",
    "  - Merge the two closest clusters\n",
    "  - Repeat until one single cluster remains\n",
    "  - Hierarchical clustering works with all types of distance type. (kMeans only with Euclidean distance)\n",
    "  - How to derive the number of cluster: divide where the largest vertical distance is.\n",
    "  - Single-linkage: distance between two clusters is defined as the shortest distance between points in the two clusters.\n",
    "  - Complete-linkage: distance between two clusters is defined as the longest distance between points in the two clusters.\n",
    "  - Avg-linkage: distance between two clusters is defined as the average distance between points in the two clusters.\n",
    "  - Wards-linkage: distance between two clusters is defined as the sum of squared differences within all clusters.\n",
    "  - Cannot be used to more than 10.000 datapoints.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Association Rules:**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Association rules are widely used to analyze retail basket or transaction data, and are intended to identify strong rules discovered in transaction data using measures of interestingness, based on the concept of strong rules.\n",
    "- Unsupervised learning\n",
    "- - Itemset = set of items\n",
    "\n",
    "  - 1-itemset: set of 1 item\n",
    "  - 2-itemset: set of 2 items\n",
    "  - ...\n",
    "\n",
    "- Frequency of an itemset = number of transactions containing that itemset\n",
    "- Monotonicity property = if an itemset is frequent, then all of its subsets are frequent\n",
    "- Support = frequency of an itemset / total number of transactions (A=>B, typically expressed as a percentage, \"occurs together\")\n",
    "- Confidence = support of itemset A and B / support of itemset A (A=>B, typically expressed as a percentage, \"conditional probability\")\n",
    "- Lift = confidence / support of itemset B (typically expressed as a percentage, \"how much more likely\") 1: No association, >1: Positive association, <1: Negative association\n",
    "-\n",
    "\n",
    "- **Apriori Algorithm:**\n",
    "\n",
    "  - This is a popular algorithm for extracting frequent 1-itemsets with applications in association rule learning. The Apriori algorithm uses breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.\n",
    "  - Given a threshold for minimum support and confidence, find all itemsets in the dataset meeting that threshold\n",
    "  - Generate rules from those itemsets meeting the minimum confidence\n",
    "\n",
    "- **Example:**\n",
    "\n",
    "Given the following buckets:\n",
    "\n",
    "- a,b,c\n",
    "- a,c\n",
    "- a,d\n",
    "- b,e,f\n",
    "\n",
    "Minimum support = 50%, minimum confidence = 70%\n",
    "\n",
    "- Frequent 1-itemsets (support count): a(3), b(2), c(2), d(1), e(1), f(1). a,b,c are the only with support >= 50%.\n",
    "- Frequent 2-itemsets (support count): ab(1), ac(2), ad(1), ae(1), af(1), bc(1), be(1), bf(1), ce(1), cf(1). ac is the only with support >= 50%.\n",
    "- Frequent 3-itemsets (support count): acb(1), ace(1), acf(1). None with support >= 50%.\n",
    "- Measure the support of the itemsets with above 50% support.\n",
    "- Confidence: a=>c = 2/3, c=>a = 2/2. Only a=>c with confidence >= 70%.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Recommender Systems:**\n",
    "\n",
    "- Recommender systems are used to suggest products, services, information to users based on their past preferences.\n",
    "- Supervised learning\n",
    "- relevance: how relevant is the item to the user\n",
    "- diversity: how diverse are the items in the recommendation\n",
    "- serendipity: how surprising are the items in the recommendation\n",
    "- matrix factorization: given a matrix, factorize it into two matrices\n",
    "- Cold start: when a new user or item enters the system, there is no information about him/her or it. This is called the cold start problem.\n",
    "  - Problem for: collaborative filtering, content based recommenders\n",
    "  - Not a problem for: knowledge based recommenders, hybrid recommenders\n",
    "- Precision-recall curve vs ROC curve: Precision-recall curve is used when the classes are very imbalanced. ROC curve is used when the classes are balanced.\n",
    "\n",
    "- **User-User Collaborative Filtering:**\n",
    "\n",
    "  - This method predicts a user's interest by collecting preferences from many users. The premise of collaborative filtering is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue.\n",
    "  - Given a user u, predict items for u by finding users similar to u, and suggest items those similar users liked\n",
    "\n",
    "- **Content based recommenders:**\n",
    "  - This method predicts a user's interest based on the similarity between the content of the items and a user profile. The premise of content-based filtering is that if a person A likes a particular item, he or she will also like an item that is similar to it.\n",
    "  - Given a user u, predict items for u by finding items similar to items u liked\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Text Analytics:**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analytics is the process of converting unstructured text data into meaningful data.\n",
    "\n",
    "- **Tokenization:** Breaking down a stream of text into words, phrases, symbols, or other meaningful elements called tokens.\n",
    "- Corpus: collection of documents\n",
    "- Document: collection of sentences and words\n",
    "- Token: a word, number, or other \"meaningful\" element\n",
    "- Vocabulary: set of unique tokens\n",
    "- Stemming = removing the suffixes of words (Porter Stemmer)\n",
    "- Word embedding: mapping words or phrases from the vocabulary to vectors of real numbers\n",
    "- Word semantics: the meaning of words\n",
    "- Word2Vec: a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW)\n",
    "-\n",
    "\n",
    "- **Lemmatization:** It involves reducing the inflected words properly ensuring that the root word belongs to the language.\n",
    "\n",
    "- **Bag of Words, TF-IDF:** These are methods to convert text data into numerical vectors. Bag of Words counts the occurrence of each word in a document while TF-IDF increases proportionally to count but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently. BOW works well with linear models.\n",
    "  - Cosine similarity\n",
    "    - Step 1: BOW Vocabulary -\n",
    "    - Unique words are: ['I', 'like', 'cats', 'and', 'dogs', 'My', 'cat', 'does', 'not', 'your', 'dog']\n",
    "    - Step 2: Vectorize sentences -\n",
    "      - Vector S1: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "      - Vector S2: [0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "    - Step 3: Calculate cosine similarity -\n",
    "      - Dot product (sum of the products of the corresponding entries of the two sequences of numbers):\n",
    "      - Dot product = (10) + (11) + (10) + (10) + (10) + (01) + (01) + (01) + (01) + (01) + (0\\*1)\n",
    "      - Dot product = 1\n",
    "    - Magnitude of vectors:\n",
    "      - Magnitude of S1 = sqrt((1^2) + (1^2) + (1^2) + (1^2) + (1^2) + (0^2) + (0^2) + (0^2) + (0^2) + (0^2) + (0^2)) = sqrt(5)\n",
    "      - Magnitude of S2 = sqrt((0^2) + (1^2) + (0^2) + (0^2) + (0^2) + (1^2) + (1^2) + (1^2) + (1^2) + (1^2) + (1^2)) = sqrt(6)\n",
    "    - Cosine similarity:\n",
    "      - Cosine similarity = Dot product / (Magnitude of S1 _ Magnitude of S2) = 1 / (sqrt(5) _ sqrt(6))\n",
    "\n",
    "Use a calculator to compute this value and round off to 2 decimal places.\n",
    "\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency):** $TF(t, d) * IDF(t)$\n",
    "  - Term frequency: TF(t, d) = Number of times term t/word appears in the document d (can contain several words and sentences)\n",
    "  - $IDF(t) = log(N / df(t))$ where N is total number of documents and df(t) is document frequency i.e., number of documents that contain term t\n",
    "  - Dimensionality: the dimensionality of the vector representing a document is equal to the size of the vocabulary of all documents. This vocabulary is formed by distinct words across all documents. Given D1 = \"the cat is gray\", D2 = \"my cat is fast\", the dimensionality of D1 is thus 6.\n",
    "  - A high TF-IDF: a high weight in TF-IDF is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents.\n",
    "- **The Jaccard similarity:** also known as the Jaccard coefficient or the Jaccard index, is a statistical measure used for comparing the similarity and diversity of sample sets. It's often used in natural language processing and information retrieval to estimate the similarity between documents or text. Given two sets, A and B, the Jaccard similarity is computed as the size of the intersection divided by the size of the union of the two sets. More formally, it can be defined as:\n",
    "\n",
    "  - J(A, B) = |A ∩ B| / |A ∪ B|\n",
    "  - Where:\n",
    "\n",
    "    - |A ∩ B| is the number of elements in both sets (i.e., the intersection of A and B)\n",
    "    - |A ∪ B| is the number of elements in either set, duplicates removed (i.e., the union of A and B)\n",
    "\n",
    "  - In the context of text analysis, sets A and B could be sets of words (or n-grams) in two different documents. The Jaccard similarity then measures how many words the documents have in common, divided by the total number of unique words across both documents.\n",
    "\n",
    "  - For example, for two sentences:\n",
    "\n",
    "    - S1: \"I like cats\"\n",
    "    - S2: \"I like dogs\"\n",
    "\n",
    "    The Jaccard similarity would be calculated as follows:\n",
    "\n",
    "    - A = {I, like, cats}\n",
    "    - B = {I, like, dogs}\n",
    "\n",
    "    - |A ∩ B| = 2 (the words \"I\" and \"like\" are common to both sets)\n",
    "    - |A ∪ B| = 4 (the set of all unique words is {I, like, cats, dogs})\n",
    "\n",
    "    Therefore, J(A, B) = 2 / 4 = 0.5\n",
    "\n",
    "    So, the Jaccard similarity of sentences S1 and S2 is 0.5, meaning they share 50% of their unique words.\n",
    "\n",
    "**Edit Distance:**\n",
    "\n",
    "- Edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Quadratic time complexity. Dynamic programming algorithm. Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Neural Networks:**\n",
    "\n",
    "- \"Stacking\" multiple logistic regression models together\n",
    "- Tensor = multidimensional array (input)\n",
    "- Fine tune the weights of the network to minimize the error\n",
    "- How? Backpropagation = gradient descent\n",
    "- Number of outputs = number of classes to be predicted\n",
    "- Softmax = activation function for the output layer. A way to get probabilities [0,1]\n",
    "  - Defined as: $softmax(x)_i = \\frac{exp(x_i)}{\\sum_{j=1}^{n}exp(x_j)}$,\n",
    "  - If we have very large numbers: $softmax(x)_i = \\frac{exp(x_i - c)}{\\sum_{j=1}^{n}exp(x_j - c)}$, where $c = max(x_i)$\n",
    "- Neuron = basic unit of a neural network, set of parameters (weights and biases) to be learnt.\n",
    "- Learning rate = how fast the model learns, how big the steps are during gradient descent\n",
    "  - Too big: overshoot the minimum, missing the optimal state\n",
    "  - Too small: takes too long to converge\n",
    "- epoch = one forward pass and one backward pass of all the training examples\n",
    "- batch size = number of training examples in one forward/backward pass\n",
    "- number of iterations = number of passes, each pass using [batch size] number of examples\n",
    "- Ways to fight overfitting:\n",
    "  - Early stopping (stop training when validation error starts to increase)\n",
    "  - Dropout (randomly remove some neurons)\n",
    "  - Regularization (L1, L2)\n",
    "\n",
    "A neural network takes in inputs, which are then processed in hidden layers using weights that are adjusted during training. The more hidden layers there are, the deeper the network is, and the more complex the patterns it can detect. The output layer then gives the final output for the given inputs. The network is trained by adjusting the weights between layers until the output error is minimized.\n",
    "\n",
    "- **Artificial Neuron:** It is the basic unit of a neural network. It gets certain inputs, processes them based on some activation function and gives an output. Input is processed using a weighted sum followed by a non-linear function (often a Sigmoid or ReLU)\n",
    "- **Multilayer Perceptron:** This is a class of feedforward artificial neural network which consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Each node in one layer connects with a certain weight to every node in the following layer. A neural network model that consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Graph Analytics:**\n",
    "\n",
    "- **Centrality (Degree, Closeness, Betweenness, Eigenvector)**: Measures to identify the most important vertices within a graph\n",
    "- **PageRank:** A link analysis algorithm that assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of \"measuring\" its relative importance within the set\n",
    "  - PageRank is a link analysis algorithm that assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of \"measuring\" its relative importance within the set. The algorithm may be applied to any collection of entities with reciprocal quotations and references. The numerical weight that it assigns to any given element E is referred to as the PageRank of E and denoted by PR(E).\n",
    "  - Video Explanation : https://www.youtube.com/watch?v=P8Kt6Abq_rM\n",
    "  - Recipe for computing page rank:\n",
    "    - Start with a graph\n",
    "    - Initialize all nodes with a page rank of 1\n",
    "    - For each iteration:\n",
    "      - For each node:\n",
    "        - Calculate the page rank of the node by summing the page rank of each node that points to it, divided by the number of nodes that point to it\n",
    "        - Update the page rank of the node\n",
    "        - Repeat until convergence (= until the page rank of each node doesn't change much)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. Dimensionality Reduction:**\n",
    "\n",
    "- Unsupervised learning technique that reduces the number of features in a dataset by obtaining a set of principal features. It can be used for data compression, feature extraction, and data visualization.\n",
    "- **Feature Selection:** Selecting a subset of the most relevant features for use in model construction\n",
    "- **Feature Extraction:** Creating new features from the existing ones\n",
    "- **Feature Transformation:** Transforming the features to a new space\n",
    "- **Feature Scaling:** Scaling the features to a specific range\n",
    "\n",
    "- **PCA (Principal Component Analysis):** A statistical procedure that uses orthogonal transformation to convert a set of observations of possibly correlated variables into a set of linearly uncorrelated variables called principal components\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
