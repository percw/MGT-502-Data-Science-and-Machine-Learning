{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheat Sheet\n",
    "\n",
    "### **1. Regression:**\n",
    "\n",
    " Regression is a statistical method that helps you understand the relationship between dependent and independent variables. It's mainly used for prediction and forecasting.\n",
    "\n",
    "   * **Multivariate linear regression:**\n",
    "       * Here, you try to predict a dependent variable based on two or more independent variables. For example, predicting a house's price based on the number of bedrooms and the size in square feet. The formula is y = β0 + β1*x1 + β2*x2 + ... + βn*xn + ε, where y is the dependent variable, x1, x2, ..., xn are the independent variables, β0, β1, ..., βn are the coefficients, and ε is the error term.\n",
    "       * $Model: y = β0 + β1*x1 + β2*x2 + ... + βn*xn + ε$\n",
    "   \n",
    "   * **Polynomial regression:**\n",
    "       * This is a form of regression where the relationship between the independent and the dependent variables is modelled as an nth degree polynomial. It is useful when the relationship is not linear but rather curvilinear. For example, predicting crop yield based on temperature, where the yield might increase with temperature to a point, but then decrease with further temperature increase.\n",
    "       * $Model: y = β0 + β1*x + β2*x^2 + ... + βd*x^d + ε$\n",
    "   \n",
    "   * **Regularization (Lasso, Ridge):**\n",
    "       * Regularization helps to prevent overfitting by adding a penalty term to the loss function. Ridge regression (L2 regularization) minimizes the sum of the squares of the coefficients, while Lasso (L1 regularization) minimizes the sum of the absolute values of the coefficients. This penalty term encourages smaller coefficients, which leads to simpler models.\n",
    "       * Lasso (L1 regularization): Sum of absolute values of coefficients\n",
    "       * Ridge (L2 regularization): Sum of squares of coefficients\n",
    "   \n",
    "   * **MAE & MSE:**\n",
    "       * are metrics to evaluate the regression model. MAE is the average of the absolute differences between the predicted and actual values. MSE is the average of the squared differences. MSE penalizes larger errors more than MAE.\n",
    "       * **MAE (Mean Absolute Error):** \n",
    "         * $1/n * Σ|yi - y'|$\n",
    "       * **MSE (Mean Squared Error):** \n",
    "         * $1/n * Σ(yi - y')^2$\n",
    "\n",
    "### **2. Classification:**\n",
    "   * Classification involves predicting a categorical label for an input.\n",
    "   * **Logistic Regression:** \n",
    "   * This is a binary classification method that models the probability that an instance belongs to the default class. For example, it can be used to predict whether an email is spam (1) or not spam (0).\n",
    "   * $log(p/1-p) = β0 + β1*x1 + β2*x2 + ... + βn*xn$\n",
    "   * **Confusion Matrix:**\n",
    "\n",
    "|       | Predicted Positive | Predicted Negative |\n",
    "|-------|--------------------|--------------------|\n",
    "| True Positive  | TP               | FN                |\n",
    "| True Negative  | FP               | TN                |\n",
    "\n",
    "Where:\n",
    "- TP = True Positives: The cases in which the model predicted Positive, and the truth is also Positive.\n",
    "- FP = False Positives (Type I error): The cases in which the model predicted Positive, but the truth is Negative.\n",
    "- FN = False Negatives (Type II error): The cases in which the model predicted Negative, but the truth is Positive.\n",
    "- TN = True Negatives: The cases in which the model predicted Negative, and the truth is also Negative.\n",
    "  \n",
    "   * This is a table that describes the performance of a classification model. It includes True Positives (actual positive and predicted positive), True Negatives (actual negative and predicted negative), False Positives (actual negative but predicted positive), and False Negatives (actual positive but predicted negative).\n",
    "   * **Accuracy:** \n",
    "       * Accuracy is the ratio of correctly predicted instances to the total instances. Overall, how often is the model correct? \n",
    "       * $(TP + TN) / (TP + TN + FP + FN)$\n",
    "   * **Precision:** \n",
    "       * Precision is the ratio of correctly predicted positive instances to the total predicted positives.  When the model predicts Positive, how often is it correct?\n",
    "       * $TP / (TP + FP)$\n",
    "   * **Recall:** \n",
    "     * Recall (or Sensitivity) is the ratio of correctly predicted positive instances to all actual positives. When the truth is Positive, how often does the model predict it?\n",
    "     * $TP / (TP + FN)$\n",
    "   * **Misclassification Rate (Error Rate):** \n",
    "     * Overall, how often is the model wrong? \n",
    "     * $Error Rate = (FP+FN) / (TP+FP+FN+TN)$\n",
    "   * **F1 Score:** \n",
    "       * F1 Score is the harmonic mean of Precision and Recall It's useful when the data has imbalanced classes.\n",
    "       * $2*(Precision*Recall)/(Precision + Recall)$\n",
    "\n",
    "**Lets use the following matrix to do calculations:**\n",
    "\n",
    "\n",
    "|       | Predicted Class 1 | Predicted Class 2 | Predicted Class 3 |\n",
    "|-------|-------------------|-------------------|-------------------|\n",
    "| True Class 1 | 50                | 10                | 5                 |\n",
    "| True Class 2 | 20                | 60                | 5                 |\n",
    "| True Class 3 | 10                | 10                | 70                |\n",
    "   \n",
    "\n",
    "1. For class 1:\n",
    "\n",
    "   * Precision for Class 1 = TP / (TP + FP) = 50 / (50 + 30) = 0.625\n",
    "   * Recall for Class 1 = TP / (TP + FN) = 50 / (50 + 15) = 0.77\n",
    "   * F1 Score for Class 1 = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.625 * 0.77) / (0.625 + 0.77) = 0.69\n",
    "   * Accuracy for Class 1 = (TP + TN) / (TP + TN + FP + FN) = (50 + 160) / (50 + 160 + 30 + 15) = 0.84\n",
    "   * Error rate for Class 1 = 1 - Accuracy = 1 - 0.84 = 0.16\n",
    "\n",
    "2. For the entire confusion matrix:\n",
    "\n",
    "   First, let's calculate overall TP, FP, FN, and TN:\n",
    "\n",
    "   * Total TP = Sum of diagonal elements = 50 + 60 + 70 = 180\n",
    "   * Total FP = Sum of all non-diagonal elements in predicted columns = 30 (from Class 1 column) + 20 (from Class 2 column) + 10 (from Class 3 column) = 60\n",
    "   * Total FN = Sum of all non-diagonal elements in true class rows = 15 (from Class 1 row) + 25 (from Class 2 row) + 20 (from Class 3 row) = 60\n",
    "   * Total TN = Sum of all elements - TP - FP - FN = 300 - 180 - 60 - 60 = 0 (this makes sense, because in multi-class problems, an instance can either be a TP, FP, or FN for a class)\n",
    "   * Accuracy for the matrix = (Total TP + Total TN) / (Total TP + Total TN + Total FP + Total FN)\n",
    "\n",
    "### **3. Clustering:**\n",
    "   * Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups.\n",
    "   * **K-Means:**\n",
    "       * This algorithm divides a set of samples into disjoint clusters. Each cluster is described by the mean of the samples in the cluster. For example, you might cluster customers into k groups based on their shopping behavior.\n",
    "       * Initialize k centroids randomly\n",
    "       * Assign each point to the nearest centroid\n",
    "       * Recalculate centroids\n",
    "       * Repeat until convergence\n",
    "   * **Hierarchical clustering:**\n",
    "       * This algorithm builds a hierarchy of clusters by creating a cluster tree or dendrogram. It can either start with all individual instances and merge them into clusters (agglomerative), or start with a single cluster and divide it up (divisive).\n",
    "       * Start with each point as a separate cluster\n",
    "       * Merge the two closest clusters\n",
    "       * Repeat until one single cluster remains\n",
    "\n",
    "### **4. Association Rules:**\n",
    "\n",
    " Association rules are widely used to analyze retail basket or transaction data, and are intended to identify strong rules discovered in transaction data using measures of interestingness, based on the concept of strong rules.\n",
    "   \n",
    "   * **Apriori Algorithm:**\n",
    "       * This is a popular algorithm for extracting frequent itemsets with applications in association rule learning. The Apriori algorithm uses breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.\n",
    "       * Given a threshold for minimum support and confidence, find all itemsets in the dataset meeting that threshold\n",
    "       * Generate rules from those itemsets meeting the minimum confidence\n",
    "\n",
    "**5. Recommender Systems:**\n",
    "Recommender systems are used to suggest products, services, information to users based on their past preferences.\n",
    "   * **User-User Collaborative Filtering:**\n",
    "       * This method predicts a user's interest by collecting preferences from many users. The premise of collaborative filtering is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue.\n",
    "       * Given a user u, predict items for u by finding users similar to u, and suggest items those similar users liked\n",
    "\n",
    "### **6. Text Analytics:**\n",
    "   Text Analytics is the process of converting unstructured text data into meaningful data.\n",
    "\n",
    "   * **Tokenization:** Breaking down a stream of text into words, phrases, symbols, or other meaningful elements called tokens.\n",
    "  \n",
    "   * **Lemmatization:** It involves reducing the inflected words properly ensuring that the root word belongs to the language.\n",
    "\n",
    "   * **Bag of Words, TF-IDF:** These are methods to convert text data into numerical vectors. Bag of Words counts the occurrence of each word in a document while TF-IDF increases proportionally to count but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently.\n",
    "     * Cosine similarity\n",
    "       * Step 1: BOW Vocabulary -\n",
    "       * Unique words are: ['I', 'like', 'cats', 'and', 'dogs', 'My', 'cat', 'does', 'not', 'your', 'dog']\n",
    "       * Step 2: Vectorize sentences -\n",
    "          * Vector S1: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "          * Vector S2: [0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "       * Step 3: Calculate cosine similarity -\n",
    "          * Dot product (sum of the products of the corresponding entries of the two sequences of numbers):\n",
    "          * Dot product = (10) + (11) + (10) + (10) + (10) + (01) + (01) + (01) + (01) + (01) + (0*1)\n",
    "          * Dot product = 1\n",
    "       * Magnitude of vectors:\n",
    "          * Magnitude of S1 = sqrt((1^2) + (1^2) + (1^2) + (1^2) + (1^2) + (0^2) + (0^2) + (0^2) + (0^2) + (0^2) + (0^2)) = sqrt(5)\n",
    "          * Magnitude of S2 = sqrt((0^2) + (1^2) + (0^2) + (0^2) + (0^2) + (1^2) + (1^2) + (1^2) + (1^2) + (1^2) + (1^2)) = sqrt(6)\n",
    "       * Cosine similarity:\n",
    "           * Cosine similarity = Dot product / (Magnitude of S1 * Magnitude of S2) = 1 / (sqrt(5) * sqrt(6))\n",
    "\n",
    "Use a calculator to compute this value and round off to 2 decimal places.\n",
    "   * **TF-IDF (Term Frequency-Inverse Document Frequency):** $TF(t, d) * IDF(t)$\n",
    "       * TF(t, d) = Number of times term t appears in document d\n",
    "       * $IDF(t) = log(N / df(t))$ where N is total number of documents and df(t) is document frequency i.e., number of documents that contain term t\n",
    "       * Dimensionality: the dimensionality of the vector representing a document is equal to the size of the vocabulary of all documents. This vocabulary is formed by distinct words across all documents. Given D1 = \"the cat is gray\", D2 = \"my cat is fast\", the dimensionality of D1 is thus 6. \n",
    "   * **The Jaccard similarity:** also known as the Jaccard coefficient or the Jaccard index, is a statistical measure used for comparing the similarity and diversity of sample sets. It's often used in natural language processing and information retrieval to estimate the similarity between documents or text. Given two sets, A and B, the Jaccard similarity is computed as the size of the intersection divided by the size of the union of the two sets. More formally, it can be defined as:\n",
    "     * J(A, B) = |A ∩ B| / |A ∪ B|\n",
    "     * Where:\n",
    "         - |A ∩ B| is the number of elements in both sets (i.e., the intersection of A and B)\n",
    "         - |A ∪ B| is the number of elements in either set, duplicates removed (i.e., the union of A and B)\n",
    "\n",
    "     * In the context of text analysis, sets A and B could be sets of words (or n-grams) in two different documents. The Jaccard similarity then measures how many words the documents have in common, divided by the total number of unique words across both documents.\n",
    "\n",
    "     * For example, for two sentences:\n",
    "         * S1: \"I like cats\"\n",
    "         * S2: \"I like dogs\"\n",
    "\n",
    "         The Jaccard similarity would be calculated as follows:\n",
    "         - A = {I, like, cats}\n",
    "         - B = {I, like, dogs}\n",
    "\n",
    "         - |A ∩ B| = 2 (the words \"I\" and \"like\" are common to both sets)\n",
    "         - |A ∪ B| = 4 (the set of all unique words is {I, like, cats, dogs})\n",
    "\n",
    "         Therefore, J(A, B) = 2 / 4 = 0.5\n",
    "\n",
    "         So, the Jaccard similarity of sentences S1 and S2 is 0.5, meaning they share 50% of their unique words.\n",
    "\n",
    "### **7. Neural Networks:**\n",
    "  A neural network takes in inputs, which are then processed in hidden layers using weights that are adjusted during training. The more hidden layers there are, the deeper the network is, and the more complex the patterns it can detect. The output layer then gives the final output for the given inputs. The network is trained by adjusting the weights between layers until the output error is minimized. \n",
    "   * **Artificial Neuron:** It is the basic unit of a neural network. It gets certain inputs, processes them based on some activation function and gives an output. Input is processed using a weighted sum followed by a non-linear function (often a Sigmoid or ReLU)\n",
    "   * **Multilayer Perceptron:** This is a class of feedforward artificial neural network which consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Each node in one layer connects with a certain weight to every node in the following layer. A neural network model that consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer\n",
    "\n",
    "### **8. Graph Analytics:**\n",
    "   * **Centrality (Degree, Closeness, Betweenness, Eigenvector)**: Measures to identify the most important vertices within a graph\n",
    "\n",
    "### **9. Dimensionality Reduction:**\n",
    "   * **PCA (Principal Component Analysis):** A statistical procedure that uses orthogonal transformation to convert a set of observations of possibly correlated variables into a set of linearly uncorrelated variables called principal components\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
